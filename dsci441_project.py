# -*- coding: utf-8 -*-
"""DSCI441_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4DpKqVu1Kt79ETUgjF6IGUagAzMCueS

Dataset comes from https://www.kaggle.com/datasets/paultimothymooney/zillow-house-price-data?select=Sale_Prices_City.csv
"""

!pip install ydata-profiling
!pip install tensorflow

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import kagglehub
import os
import warnings

from ydata_profiling import ProfileReport
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from IPython.display import clear_output, display, HTML

# Ignore all warnings
warnings.filterwarnings("ignore")


clear_output()

# Download data
path = kagglehub.dataset_download("paultimothymooney/zillow-house-price-data")

print("Files in the dataset:")
for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))

csv_path = os.path.join(path, "City_MedianRentalPrice_AllHomes.csv")
df = pd.read_csv(csv_path)
print(df.head())

profile = ProfileReport(df, title="Housing Prices")
profile

# remove rows with NaN
df_cleaned = df.dropna()
print("DataFrame after removing rows with any NaN values:")
print(df_cleaned.head())
data = df_cleaned

# Remove location identifier since only one city has data for each month/year
data.drop('State',axis=1,inplace=True)
data.drop('CountyName',axis=1,inplace=True)
data.drop('SizeRank',axis=1,inplace=True)
data.drop('RegionName',axis=1,inplace=True)
data.drop('Metro',axis=1,inplace=True)
data.drop('Unnamed: 0',axis=1,inplace=True)

df = data.reset_index(drop=True)

print(data)

# reshape data to have rows correspond to each time, with features being the time and price
reshaped_data = []

# Loop through each column to get feature dates
for column in df.columns:
  year, month = map(int, column.split('-'))

        # Loop through each row to extract the price for the current date
  for index, row in df.iterrows():
    price = row[column]

            # Append the data to the reshaped_data list
    reshaped_data.append({
      'Price': price,
      'Year': year,
      'Month': month,
      'Year-Month': f'{year}-{month}'
      })

reshaped_df = pd.DataFrame(reshaped_data)

# Add a time index
reshaped_df['TimeIndex'] = (reshaped_df['Year'] - reshaped_df['Year'].min()) * 12 + (reshaped_df['Month'] - reshaped_df['Month'].min())

# Sort data by month/year
full_df = reshaped_df.sort_values(by=['Year', 'Month']).reset_index(drop=True)

print("Reshaped DataFrame:")
print(full_df)

# Plot the price over time
plt.figure(figsize=(18, 6))
plt.plot(full_df['Year-Month'], full_df['Price'])
plt.xlabel('Year')
plt.ylabel('Price')
x_ticks = np.arange(0,120,12)
plt.xticks(x_ticks)
plt.title('Time Series Plot of Price Over Time')
plt.show()

import pandas as pd
import numpy as np
import statsmodels.api as sm

# Split data into training (2010-02 to 2017-12) and test (2018-01 to 2019-12)
train = full_df[(full_df['Year'] < 2018) | ((full_df['Year'] == 2017) & (full_df['Month'] <= 12))]
test = full_df[(full_df['Year'] > 2017) | ((full_df['Year'] == 2018) & (full_df['Month'] >= 1))]

# Define features and target
X_train = train[['Year', 'Month', 'TimeIndex']]
y_train = train['Price']

# add constant
X_train = sm.add_constant(X_train)

# Fit OLS model
model = sm.OLS(y_train, X_train)
results = model.fit()

# Model Summary
print(results.summary())

# Prediction test
X_test = test[['Year', 'Month', 'TimeIndex']]
X_test = sm.add_constant(X_test)

predictions = results.predict(X_test)
test['Predicted_Price'] = predictions

print("\n\nPredictions for 2018-2019:")
print(test[['Year', 'Month', 'Price', 'Predicted_Price']])

y_test = test['Price']
y_pred = test['Predicted_Price']

# model evaluation
mse = mean_squared_error(y_test, y_pred)
print(f"\n\nMean Squared Error (MSE): {mse}")
mape = mean_absolute_percentage_error(y_test, y_pred)
print("Mean Absolute Percentage Error:", mape)
MAE = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", MAE)

plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['Price'],color='red')
plt.plot(test['Year-Month'], test['Predicted_Price'], color='blue')
plt.xlabel('Year')
plt.ylabel('Price')
x_ticks = np.arange(0,24,2)
plt.xticks(x_ticks)
plt.title('Time Series Plot')
plt.show()

# Step 1: Calculate the percentage of NaN values in each column
nan_percentage = df.isna().mean() * 100

# Step 2: Filter columns with more than 20% NaN values
columns_with_high_nans = nan_percentage[nan_percentage > 1].index.tolist()

print("Columns with more than 20% NaN values:", columns_with_high_nans)

# Step 3: Drop these columns
df_cleaned = df.drop(columns=columns_with_high_nans)

print("DataFrame after dropping columns with more than 20% NaN values:")
print(df_cleaned)

profile = ProfileReport(data, title="Housing Prices")
profile