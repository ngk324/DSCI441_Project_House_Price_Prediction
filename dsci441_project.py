# -*- coding: utf-8 -*-
"""DSCI441_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hQrX4zq9cYPEuP9WmDAkrXfONMvxWNI6

ZHVI dataset comes from https://www.kaggle.com/datasets/paultimothymooney/zillow-house-price-data?select=Sale_Prices_City.csv

Unemployment rate dataset comes from https://www.kaggle.com/datasets/axeltorbenson/unemployment-data-19482021

Inflation Rate(CPI) Dataset https://www.kaggle.com/datasets/varpit94/us-inflation-data-updated-till-may-2021

Interest rate dataset https://www.kaggle.com/datasets/raoofiali/us-interest-rate-weekly

GDP Growth Rate dataset https://www.kaggle.com/datasets/rajkumarpandey02/economy-of-the-united-states
"""

#!pip install ydata-profiling
#!pip install tensorflow

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
import kagglehub
import math
import os
import warnings

#from ydata_profiling import ProfileReport
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error, r2_score

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
#from tensorflow.keras.models import Sequential
#from tensorflow.keras.layers import Dense
from IPython.display import clear_output, display, HTML

warnings.filterwarnings("ignore")
clear_output()

"""Adding Housing Data"""

# Download housing data
path = kagglehub.dataset_download("paultimothymooney/zillow-house-price-data")

print("Files in the dataset:")
for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))

csv_path = os.path.join(path, "City_Zhvi_AllHomes.csv")
df = pd.read_csv(csv_path)
print(df.head())

# remove rows with NaN
df_cleaned = df.dropna()
print("DataFrame after removing rows with any NaN values:")
print(df_cleaned.head())
data = df_cleaned

# Remove location identifier since only one city has data for each month/year
data.drop('State',axis=1,inplace=True)
data.drop('CountyName',axis=1,inplace=True)
data.drop('SizeRank',axis=1,inplace=True)
data.drop('Metro',axis=1,inplace=True)
data.drop('Unnamed: 0',axis=1,inplace=True)
data.drop('RegionID',axis=1,inplace=True)
data.drop('RegionType',axis=1,inplace=True)
data.drop('StateName',axis=1,inplace=True)
data = data.reset_index(drop=True)

# Select single city (New York)
data = data[data['RegionName']=='New York']
data.drop('RegionName',axis=1,inplace=True)
print(data)

"""Adding Interest Rate Data"""

path = kagglehub.dataset_download("raoofiali/us-interest-rate-weekly")

print("Files in the dataset:")
for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))

xlsx_path = os.path.join(path, "Us-Interest Rate-Weekly.xlsx")
ir_df = pd.read_excel(xlsx_path)
ir_df.drop('Unnamed: 0',axis=1,inplace=True)
print(ir_df.head())
print(ir_df.tail())

# convert date format
ir_df['Date'] = pd.to_datetime(ir_df['Date'])

# Filter to include only rows between January 1996 and March 2020 to match housing data
start_date = pd.to_datetime('1996-01-01')
end_date = pd.to_datetime('2020-03-31')
filtered_ir_df = ir_df[(ir_df['Date'] >= start_date) & (ir_df['Date'] <= end_date)]

# Resample the data to get the monthly average
ir_df = filtered_ir_df.resample('M', on='Date').mean().reset_index()

# create time index
ir_df['Year'] = ir_df['Date'].dt.year
ir_df['Month'] = ir_df['Date'].dt.month
ir_df['TimeIndex'] = (ir_df['Year'] - ir_df['Year'].min()) * 12 + (ir_df['Month'] - ir_df['Month'].min())
ir_df.drop('Date',axis=1,inplace=True)

print(ir_df.head())
print(ir_df.tail())

"""Adding Inflation Rate Data"""

path = kagglehub.dataset_download("varpit94/us-inflation-data-updated-till-may-2021")

print("Files in the dataset:")
for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))

csv_path = os.path.join(path, "US CPI.csv")
cpi_df = pd.read_csv(csv_path)

print(cpi_df.head())
print(cpi_df.tail())

cpi_df['Yearmon'] = pd.to_datetime(cpi_df['Yearmon'], format='%d-%m-%Y')

start_date = pd.to_datetime('1996-01-01')
end_date = pd.to_datetime('2020-03-31')
filtered_cpi_df = cpi_df[(cpi_df['Yearmon'] >= start_date) & (cpi_df['Yearmon'] <= end_date)]
filtered_cpi_df = filtered_cpi_df.reset_index(drop=True)

filtered_cpi_df['Year'] = filtered_cpi_df['Yearmon'].dt.year
filtered_cpi_df['Month'] = filtered_cpi_df['Yearmon'].dt.month
filtered_cpi_df['TimeIndex'] = (filtered_cpi_df['Year'] - filtered_cpi_df['Year'].min()) * 12 + (filtered_cpi_df['Month'] - filtered_cpi_df['Month'].min())
filtered_cpi_df = filtered_cpi_df.reset_index(drop=True)

print(filtered_cpi_df)

"""Adding Unemployment rate data"""

# download unemployment rate data
path = kagglehub.dataset_download("axeltorbenson/unemployment-data-19482021")

print("Files in the dataset:")
for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))

# Load CSV file
csv_path = os.path.join(path, "unemployment_rate_data.csv")
un_df = pd.read_csv(csv_path)

print(un_df.head())
print(un_df.tail())

# select same range of dates of housing data and only the overall unemployment rate
un_df = un_df.iloc[576:576+291][['unrate','date']]
un_df = un_df.reset_index(drop=True)

# Convert the date column to get specific year and month feature
un_df['date'] = pd.to_datetime(un_df['date'])
un_df['Year'] = un_df['date'].dt.year
un_df['Month'] = un_df['date'].dt.month
un_df['TimeIndex'] = (un_df['Year'] - un_df['Year'].min()) * 12 + (un_df['Month'] - un_df['Month'].min())
un_df.drop('date',axis=1,inplace=True)

"""Adding GDP Growth %"""

# Download data
path = kagglehub.dataset_download("rajkumarpandey02/economy-of-the-united-states")

print("Path to dataset files:", path)

print("Files in the dataset:")
for root, dirs, files in os.walk(path):
    for file in files:
        print(os.path.join(root, file))

csv_path = os.path.join(path, "Economy of the United States.csv")
gdp_df = pd.read_csv(csv_path)

print(gdp_df.head())
print(gdp_df.tail())

gdp_df = gdp_df[gdp_df['Year'] >= 1996]
gdp_df = gdp_df[gdp_df['Year'] <= 2020]
gdp_df = gdp_df.reset_index(drop=True)
gdp_df = gdp_df[['Year','GDP growth (real)']]

gdp_df['GDP growth (real)'] = gdp_df['GDP growth (real)'].str.replace('%', '')
gdp_df['GDP Growth'] = pd.to_numeric(gdp_df['GDP growth (real)'])
gdp_df.drop('GDP growth (real)',axis=1,inplace=True)

# add instance for each month
gdp_df = gdp_df.loc[gdp_df.index.repeat(12)].reset_index(drop=True)
gdp_df['Month'] = (gdp_df.groupby('Year').cumcount() % 12) + 1
gdp_df = gdp_df.iloc[:-9]

print(gdp_df.head())
print(gdp_df.tail())

# reshape data to have rows correspond to each time, with features being the time, price, and unemployment rate
reshaped_data = []

# Loop through each column to get feature dates
for column in data.columns:
  year, month,day = map(int, column.split('-'))

  # Loop through each row to get price for the current date
  for index, row in data.iterrows():
   zhvi = row[column]

   reshaped_data.append({
      'ZHVI': zhvi,
      'Year': year,
      'Month': month,
      'Year-Month': f'{year}-{month}'
      })

reshaped_df = pd.DataFrame(reshaped_data)

# Add a time index
reshaped_df['TimeIndex'] = (reshaped_df['Year'] - reshaped_df['Year'].min()) * 12 + (reshaped_df['Month'] - reshaped_df['Month'].min())

# Sort data by month/year
full_df = reshaped_df.sort_values(by=['Year', 'Month']).reset_index(drop=True)
full_df['Unemployment Rate'] = un_df['unrate']
full_df['CPI'] = filtered_cpi_df['CPI']
full_df['Interest Rate'] = ir_df['Value']
full_df['GDP Growth'] = gdp_df['GDP Growth']
print("Reshaped DataFrame:")
print(full_df)

# Create figure and primary axis
fig, ax1 = plt.subplots(figsize=(18, 6))

# Plot the first ZHVI dataset
ax1.plot(full_df['Year-Month'], full_df['ZHVI'], color='blue', label='ZHVI')
ax1.set_xlabel('DATE')
ax1.set_ylabel('ZHVI', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# Create a second axis sharing the same x-axis
ax2 = ax1.twinx()

# Plot the Unemployment Rate data
ax2.plot(un_df['TimeIndex'], un_df['unrate'], color='red', label='Unemployment Rate')
ax2.set_ylabel('Unemployment Rate', color='red')
ax2.tick_params(axis='y', labelcolor='red')

x_ticks = np.arange(0, 290, 24)
ax1.set_xticks(x_ticks)

plt.title('Time Series Plot of ZHVI and Unemployment Rate Over Time')

# legend
ax1.legend(loc='upper left')
ax2.legend(loc='upper right')

plt.show()

# Create figure and primary axis
fig, ax1 = plt.subplots(figsize=(18, 6))

# Plot the first ZHVI dataset
ax1.plot(full_df['Year-Month'], full_df['ZHVI'], color='blue', label='ZHVI')
ax1.set_xlabel('DATE')
ax1.set_ylabel('ZHVI', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# Create a second axis sharing the same x-axis
ax2 = ax1.twinx()

# Plot the Unemployment Rate data
ax2.plot(filtered_cpi_df['TimeIndex'], filtered_cpi_df['CPI'], color='red', label='CPI')
ax2.set_ylabel('CPI', color='red')
ax2.tick_params(axis='y', labelcolor='red')

x_ticks = np.arange(0, 290, 24)
ax1.set_xticks(x_ticks)

plt.title('Time Series Plot of ZHVI and CPI Over Time')

# legend
ax1.legend(loc='upper left')
ax2.legend(loc='upper right')

plt.show()

# Create figure and primary axis
fig, ax1 = plt.subplots(figsize=(18, 6))

# Plot the first ZHVI dataset
ax1.plot(full_df['Year-Month'], full_df['ZHVI'], color='blue', label='ZHVI')
ax1.set_xlabel('DATE')
ax1.set_ylabel('ZVHI', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# Create a second axis sharing the same x-axis
ax2 = ax1.twinx()

# Plot the Unemployment Rate data
ax2.plot(ir_df['TimeIndex'], ir_df['Value'], color='red', label='Interest Rate')
ax2.set_ylabel('Interest Rate', color='red')
ax2.tick_params(axis='y', labelcolor='red')

x_ticks = np.arange(0, 290, 24)
ax1.set_xticks(x_ticks)

plt.title('Time Series Plot of ZHVI and Interest Rate Over Time')

# legend
ax1.legend(loc='upper left')
ax2.legend(loc='upper right')

plt.show()

# Create figure and primary axis
fig, ax1 = plt.subplots(figsize=(18, 6))

# Plot the first ZHVI dataset
ax1.plot(full_df['Year-Month'], full_df['ZHVI'], color='blue', label='ZHVI')
ax1.set_xlabel('DATE')
ax1.set_ylabel('ZHVI', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# Create a second axis sharing the same x-axis
ax2 = ax1.twinx()

# Plot the Unemployment Rate data
ax2.plot(ir_df['TimeIndex'], gdp_df['GDP Growth'], color='red', label='GDP Growth Rate')
ax2.set_ylabel('GDP Growth Rate', color='red')
ax2.tick_params(axis='y', labelcolor='red')

x_ticks = np.arange(0, 290, 24)
ax1.set_xticks(x_ticks)

plt.title('Time Series Plot of ZHVI and GDP Growth Rate Over Time')

# legend
ax1.legend(loc='upper left')
ax2.legend(loc='upper right')

plt.show()

# Split data into training and test
train = full_df[(full_df['Year'] < 2014) | ((full_df['Year'] == 2013) & (full_df['Month'] <= 12))]
test = full_df[(full_df['Year'] > 2013) | ((full_df['Year'] == 2014) & (full_df['Month'] >= 1))]

# Define features and target
X_train = train[['Year', 'Month', 'TimeIndex', 'Unemployment Rate', 'CPI', 'Interest Rate', 'GDP Growth']]
y_train = train['ZHVI']

# Prediction test
X_test = test[['Year', 'Month', 'TimeIndex','Unemployment Rate', 'CPI','Interest Rate', 'GDP Growth']]

# add polynomial features and scale
scaler = StandardScaler()
poly = PolynomialFeatures(degree=2)

X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

X_train_scaled = scaler.fit_transform(X_train_poly)
X_test_scaled = scaler.transform(X_test_poly)

# add constant
X_train_scaled = sm.add_constant(X_train_scaled)
X_test_scaled = sm.add_constant(X_test_scaled)

# Fit OLS model
model = sm.OLS(y_train, X_train_scaled)
results = model.fit()

predictions = results.predict(X_test_scaled)
test['Predicted_ZHVI'] = predictions

y_test = test['ZHVI']
y_pred = test['Predicted_ZHVI']
OLS_pred = test['Predicted_ZHVI']

# model evaluation
rmse = math.sqrt(mean_squared_error(y_test, y_pred))
print(f"OLS Root Mean Squared Error (RMSE): {rmse}")
mape = mean_absolute_percentage_error(y_test, y_pred)
print("OLS Mean Absolute Percentage Error(MAPE):", mape)
MAE = mean_absolute_error(y_test, y_pred)
print("OLS Mean Absolute Error(MAE):", MAE)
r2 = r2_score(y_pred,y_test)
print(f"OLS R-squared(R^2): {r2}")

# Plot truth vs prediction
plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['ZHVI'], color='red', label='Truth (ZHVI)')
plt.plot(test['Year-Month'], test['Predicted_ZHVI'], color='blue', label='Predicted ZHVI')
plt.xlabel('Year')
plt.ylabel('Price')
x_ticks = np.arange(0, 80, 6)
plt.xticks(x_ticks)
plt.title('Time Series Plot: Truth vs OLS Predicted ZHVI')
plt.legend(loc='upper left')
plt.show()

# Get feature names
feature_names = ['const'] + list(poly.get_feature_names_out(X_train.columns))

# Create dataframe to store coefficients and feature names
coefficients_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': results.params
})

# sort features by coeff magnitude
coefficients_df['Absolute_Coefficient'] = np.abs(coefficients_df['Coefficient'])
coefficients_df = coefficients_df.sort_values(by='Absolute_Coefficient', ascending=False)

print("Sorted Top 10 OLS Regression Coefficients:")
print(coefficients_df[0:10])

# plot coeff
plt.figure(figsize=(10, 6))
plt.barh(coefficients_df['Feature'][:10], coefficients_df['Absolute_Coefficient'][:10], color='skyblue')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.title('Top 10 Most Important Features (OLS Regression)')
plt.gca().invert_yaxis()
plt.show()

# Split data into training and test
train = full_df[(full_df['Year'] < 2014) | ((full_df['Year'] == 2013) & (full_df['Month'] <= 12))]
test = full_df[(full_df['Year'] > 2013) | ((full_df['Year'] == 2014) & (full_df['Month'] >= 1))]

# Define features and target
X_train = train[['Year', 'Month', 'TimeIndex', 'Unemployment Rate', 'CPI','Interest Rate', 'GDP Growth']]
y_train = train['ZHVI']

# Prepare test data for prediction
X_test = test[['Year', 'Month', 'TimeIndex', 'Unemployment Rate', 'CPI','Interest Rate', 'GDP Growth']]

# add polynomial features and scale
scaler = StandardScaler()
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

X_train_scaled = scaler.fit_transform(X_train_poly)
X_test_scaled = scaler.transform(X_test_poly)

# Fit Lasso regression model
alphas = [0.001,0.005,0.01, 0.05, 0.1, 0.5, 1, 2, 3, 5, 10,25,50,75,100,150]
results = []
lowest_alpha = alphas[0]
lowest_mape = float('inf')

for alpha in alphas:
  lasso_model = Lasso(alpha=alpha)
  lasso_model.fit(X_train_scaled, y_train)

  # Predict
  predictions = lasso_model.predict(X_test_scaled)
  test['Predicted_ZHVI'] = predictions

  # Model evaluation
  y_test = test['ZHVI']
  y_pred = test['Predicted_ZHVI']
  lasso_pred = test['Predicted_ZHVI']

  mape = mean_absolute_percentage_error(y_test, y_pred)
  results.append(mape)
  if mape < lowest_mape:
    lowest_mape = mape
    lowest_alpha = alpha

print("Lowest MAPE:", lowest_mape)
print("Lowest Alpha:", lowest_alpha)

# Plot hyperparameter tuning
plt.plot(alphas, results, marker='o')
plt.xscale('log')

plt.xlabel('Alpha (log scale)')
plt.ylabel('MAPE')
plt.title('Lasso Regression Alpha Tuning - MAPE vs Alpha (Logarithmic X-axis)')

plt.show()

alpha = lowest_alpha

lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X_train_scaled, y_train)

# Predict
predictions = lasso_model.predict(X_test_scaled)
test['Predicted_ZHVI'] = predictions

# Model evaluation
y_test = test['ZHVI']
y_pred = test['Predicted_ZHVI']
lasso_pred = test['Predicted_ZHVI']

lasso_rmse = math.sqrt(mean_squared_error(y_test, y_pred))
print(f"\n\nLasso Root Mean Squared Error (RMSE): {lasso_rmse}")
lasso_mape = mean_absolute_percentage_error(y_test, y_pred)
print("Lasso Mean Absolute Percentage Error (MAPE):", lasso_mape)
lasso_MAE = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error (MAE):", lasso_MAE)
lasso_r2 = r2_score(y_pred,y_test)
print(f"R-squared(R^2): {lasso_r2}")

# Plot truth vs prediction
plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['ZHVI'], color='red', label='Truth (ZHVI)')
plt.plot(test['Year-Month'], test['Predicted_ZHVI'], color='blue', label='Predicted ZHVI')
plt.xlabel('Date')
plt.ylabel('ZHVI')
x_ticks = np.arange(0, 80, 6)
plt.xticks(x_ticks)
plt.title('Time Series Plot: Truth vs Lasso Regression Predicted ZHVI')
plt.legend(loc='upper left')
plt.show()

# Get feature names
feature_names = poly.get_feature_names_out(X_train.columns)

# Create dataframe to store coefficients and feature names
coefficients_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': lasso_model.coef_
})

# sort features by coeff magnitude
coefficients_df['Absolute_Coefficient'] = np.abs(coefficients_df['Coefficient'])
coefficients_df = coefficients_df.sort_values(by='Absolute_Coefficient', ascending=False)

print("Sorted Top 10 Lasso Regression Coefficients:")
print(coefficients_df[0:10])

# plot coeff
plt.figure(figsize=(10, 6))
plt.barh(coefficients_df['Feature'][:10], coefficients_df['Absolute_Coefficient'][:10], color='skyblue')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.title('Top 10 Most Important Features (Lasso Regression)')
plt.gca().invert_yaxis()
plt.show()

# Split data into training and test
train = full_df[(full_df['Year'] < 2014) | ((full_df['Year'] == 2013) & (full_df['Month'] <= 12))]
test = full_df[(full_df['Year'] > 2013) | ((full_df['Year'] == 2014) & (full_df['Month'] >= 1))]

# Define features and target
X_train = train[['Year', 'Month', 'TimeIndex', 'Unemployment Rate', 'CPI','Interest Rate', 'GDP Growth']]
y_train = train['ZHVI']

X_test = test[['Year', 'Month', 'TimeIndex', 'Unemployment Rate', 'CPI','Interest Rate', 'GDP Growth']]

# add polynomial features and scale
scaler = StandardScaler()
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

X_train_scaled = scaler.fit_transform(X_train_poly)
X_test_scaled = scaler.transform(X_test_poly)

# Fit Ridge regression model
alphas = [0.001,0.005,0.01, 0.05, 0.075, 0.1, 0.25, 0.35, 0.5, 1, 2, 3, 5, 10]

lowest_alpha = alphas[0]
lowest_mape = float('inf')
results = []
for alpha in alphas:
  ridge_model = Ridge(alpha=alpha)
  ridge_model.fit(X_train_scaled, y_train)

  # Predict
  predictions = ridge_model.predict(X_test_scaled)
  test['Predicted_ZHVI'] = predictions

  # Model evaluation
  y_test = test['ZHVI']
  y_pred = test['Predicted_ZHVI']
  ridge_pred = test['Predicted_ZHVI']

  mape = mean_absolute_percentage_error(y_test, y_pred)
  results.append(mape)
  if mape < lowest_mape:
    lowest_mape = mape
    lowest_alpha = alpha

print("Lowest MAPE:", lowest_mape)
print("Lowest Alpha:", lowest_alpha)

# Plot hyperparameter tuning
plt.plot(alphas, results, marker='o')
plt.xscale('log')

plt.xlabel('Alpha (log scale)')
plt.ylabel('MAPE')
plt.title('Ridge Regression Alpha Tuning - MAPE vs Alpha (Logarithmic X-axis)')

plt.show()

alpha = lowest_alpha
ridge_model = Ridge(alpha=alpha)
ridge_model.fit(X_train_scaled, y_train)

# Predict
predictions = ridge_model.predict(X_test_scaled)
test['Predicted_ZHVI'] = predictions

# Model evaluation
y_test = test['ZHVI']
y_pred = test['Predicted_ZHVI']
ridge_pred = test['Predicted_ZHVI']

ridge_rmse = math.sqrt(mean_squared_error(y_test, y_pred))
print(f"\n\nRR Root Mean Squared Error (RMSE): {ridge_rmse}")
ridge_mape = mean_absolute_percentage_error(y_test, y_pred)
print("RR Mean Absolute Percentage Error(MAPE):", ridge_mape)
ridge_MAE = mean_absolute_error(y_test, y_pred)
print("RR Mean Absolute Error(MAE):", ridge_MAE)
ridge_r2 = r2_score(y_pred,y_test)
print(f"R-squared(R^2): {ridge_r2}")

# Plot truth vs prediction
plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['ZHVI'], color='red', label='Truth (ZHVI)')
plt.plot(test['Year-Month'], test['Predicted_ZHVI'], color='blue', label='Predicted ZHVI')
plt.xlabel('Year')
plt.ylabel('ZHVI')
x_ticks = np.arange(0, 80, 6)
plt.xticks(x_ticks)
plt.title('Time Series Plot: Truth vs Ridge Regression Predicted ZHVI')
plt.legend(loc='upper left')
plt.show()

# Get feature names
feature_names = poly.get_feature_names_out(X_train.columns)

# Create dataframe to store coefficients and feature names
coefficients_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': ridge_model.coef_
})

# sort features by coeff magnitude
coefficients_df['Absolute_Coefficient'] = np.abs(coefficients_df['Coefficient'])
coefficients_df = coefficients_df.sort_values(by='Absolute_Coefficient', ascending=False)

print("Sorted Top 10 Ridge Regression Coefficients:")
print(coefficients_df[0:10])

# plot coeff
plt.figure(figsize=(10, 6))
plt.barh(coefficients_df['Feature'][:10], coefficients_df['Absolute_Coefficient'][:10], color='skyblue')
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.title('Top 10 Most Important Features (Ridge Regression)')
plt.gca().invert_yaxis()
plt.show()

plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['ZHVI'], color='black', label='True ZHVI')
plt.plot(test['Year-Month'], OLS_pred, color='blue', label='OLS Predicted ZHVI')
plt.plot(test['Year-Month'], lasso_pred, color='green', label='Lasso Predicted ZHVI')
plt.plot(test['Year-Month'], ridge_pred, color='red', label='Ridge Predicted ZHVI')

plt.xlabel('Date')
plt.ylabel('ZHVI')
x_ticks = np.arange(0, 80, 6)
plt.xticks(x_ticks)
plt.title('Time Series Plot of OLS, Ridge, and Lasso Regression: True vs Predicted ZHVI')

plt.legend(loc='upper left')

plt.show()

"""milestone 2"""

!pip install keras-tuner

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import keras_tuner as kt
import math
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error, r2_score

# Preprocess data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the model building function for Keras Tuner
def build_model(hp):

    model = Sequential()

    # Tune the number of units in the first Dense layer
    model.add(Dense(
        units=hp.Int('units_1', min_value=64, max_value=256, step=64),
        activation='relu',
        input_shape=(X_train_scaled.shape[1],)
    ))
    model.add(Dropout(
        rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)
    ))

    # Tune the number of hidden layers (1-3)
    for i in range(hp.Int('num_layers', 1, 3)):
        model.add(Dense(
            units=hp.Int(f'units_{i+2}', min_value=32, max_value=128, step=32),
            activation='relu')
        )
        model.add(Dropout(
            rate=hp.Float(f'dropout_{i+2}', min_value=0.0, max_value=0.5, step=0.1)
        ))

    model.add(Dense(1))

    # Tune the learning rate
    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

    return model

# Set up the tuner
tuner = kt.RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=20,
    executions_per_trial=2,
    directory='keras_tuner',
    project_name='zhvi_prediction'
)

# Early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# Perform hyperparameter search
tuner.search(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)

# Get the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal hyperparameters are:
- Units in first layer: {best_hps.get('units_1')}
- Number of hidden layers: {best_hps.get('num_layers')}
- Learning rate: {best_hps.get('learning_rate')}
- Dropout rate (first layer): {best_hps.get('dropout_1')}
""")

# Build the model with the best hyperparameters
best_model = tuner.hypermodel.build(best_hps)

# Train the best model
history = best_model.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)

# Plot training history
plt.figure(figsize=(12, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.title('Neural Network Training History')
plt.legend()
plt.show()

# Predictions
nn_predictions = best_model.predict(X_test_scaled).flatten()
test['NN_Predicted_ZHVI'] = nn_predictions

# Model evaluation
nn_rmse = math.sqrt(mean_squared_error(y_test, nn_predictions))
print(f"Neural Network Root Mean Squared Error (RMSE): {nn_rmse}")
nn_mape = mean_absolute_percentage_error(y_test, nn_predictions)
print("Neural Network Mean Absolute Percentage Error (MAPE):", nn_mape)
nn_mae = mean_absolute_error(y_test, nn_predictions)
print("Neural Network Mean Absolute Error (MAE):", nn_mae)
nn_r2 = r2_score(y_test, nn_predictions)
print(f"Neural Network R-squared (R^2): {nn_r2}")

# Plot truth vs prediction
plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['ZHVI'], color='red', label='Truth (ZHVI)')
plt.plot(test['Year-Month'], test['NN_Predicted_ZHVI'], color='purple', label='Neural Network Predicted ZHVI')
plt.xlabel('Date')
plt.ylabel('ZHVI')
x_ticks = np.arange(0, 80, 6)
plt.xticks(x_ticks)
plt.title('Time Series Plot: Truth vs Neural Network Predicted ZHVI (Optimized)')
plt.legend(loc='upper left')
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation
from tensorflow.keras.optimizers import Adam
import random
import tensorflow as tf
import numpy as np

# set seed for easy reproducibility of different configurations
random.seed(158)


# preprocess data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build neural network
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1)
])

optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

# Train the model
history = model.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# Predictions
nn_predictions = model.predict(X_test_scaled).flatten()
test['NN_Predicted_ZHVI'] = nn_predictions

# Model evaluation
nn_rmse = math.sqrt(mean_squared_error(y_test, nn_predictions))
print(f"Neural Network Root Mean Squared Error (RMSE): {nn_rmse}")
nn_mape = mean_absolute_percentage_error(y_test, nn_predictions)
print("Neural Network Mean Absolute Percentage Error (MAPE):", nn_mape)
nn_mae = mean_absolute_error(y_test, nn_predictions)
print("Neural Network Mean Absolute Error (MAE):", nn_mae)
nn_r2 = r2_score(y_test, nn_predictions)
print(f"Neural Network R-squared (R^2): {nn_r2}")

# Plot truth vs prediction
plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['ZHVI'], color='red', label='Truth (ZHVI)')
plt.plot(test['Year-Month'], test['NN_Predicted_ZHVI'], color='purple', label='Neural Network Predicted ZHVI')
plt.xlabel('Date')
plt.ylabel('ZHVI')
x_ticks = np.arange(0, 80, 6)
plt.xticks(x_ticks)
plt.title('Time Series Plot: Truth vs Neural Network Predicted ZHVI')
plt.legend(loc='upper left')
plt.show()

# Combined Model Comparison Plot
plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['ZHVI'], color='black', label='True ZHVI')
plt.plot(test['Year-Month'], OLS_pred, color='blue', label='OLS Predicted ZHVI')
plt.plot(test['Year-Month'], lasso_pred, color='green', label='Lasso Predicted ZHVI')
plt.plot(test['Year-Month'], ridge_pred, color='red', label='Ridge Predicted ZHVI')
plt.plot(test['Year-Month'], test['NN_Predicted_ZHVI'], color='purple', label='Neural Network Predicted ZHVI')

plt.xlabel('Date')
plt.ylabel('ZHVI')
x_ticks = np.arange(0, 80, 6)
plt.xticks(x_ticks)
plt.title('Time Series Plot: True vs All Model Predictions')
plt.legend(loc='upper left')
plt.show()

# Model Performance Comparison Table
model_comparison = pd.DataFrame({
    'Model': ['OLS', 'Lasso', 'Ridge', 'Neural Network'],
    'RMSE': [rmse, lasso_rmse, ridge_rmse, nn_rmse],
    'MAPE': [mape, lasso_mape, ridge_mape, nn_mape],
    'MAE': [MAE, lasso_MAE, ridge_MAE, nn_mae],
    'R2': [r2, lasso_r2, ridge_r2, nn_r2]
})

print("\nModel Performance Comparison:")
print(model_comparison.sort_values('RMSE'))

from sklearn.ensemble import RandomForestRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import GridSearchCV

# Random Forest Model

# Split data into training and test (same as before)
train = full_df[(full_df['Year'] < 2014) | ((full_df['Year'] == 2013) & (full_df['Month'] <= 12))]
test = full_df[(full_df['Year'] > 2013) | ((full_df['Year'] == 2014) & (full_df['Month'] >= 1))]

# Define features and target
X_train = train[['Year', 'Month', 'TimeIndex', 'Unemployment Rate', 'CPI', 'Interest Rate', 'GDP Growth']]
y_train = train['ZHVI']
X_test = test[['Year', 'Month', 'TimeIndex', 'Unemployment Rate', 'CPI', 'Interest Rate', 'GDP Growth']]
y_test = test['ZHVI']

# add polynomial features and scale
scaler = StandardScaler()
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

X_train_scaled = scaler.fit_transform(X_train_poly)
X_test_scaled = scaler.transform(X_test_poly)

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                          cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')
grid_search.fit(X_train_scaled, y_train)

# Best model
best_rf = grid_search.best_estimator_
print(f"Best Random Forest parameters: {grid_search.best_params_}")

# Predictions
rf_predictions = best_rf.predict(X_test_scaled)
test['RF_Predicted_ZHVI'] = rf_predictions

# Model evaluation
rf_rmse = math.sqrt(mean_squared_error(y_test, rf_predictions))
print(f"Random Forest Root Mean Squared Error (RMSE): {rf_rmse}")
rf_mape = mean_absolute_percentage_error(y_test, rf_predictions)
print("Random Forest Mean Absolute Percentage Error (MAPE):", rf_mape)
rf_mae = mean_absolute_error(y_test, rf_predictions)
print("Random Forest Mean Absolute Error (MAE):", rf_mae)
rf_r2 = r2_score(y_test, rf_predictions)
print(f"Random Forest R-squared (R^2): {rf_r2}")

# Feature importance
feature_importance = pd.DataFrame({
    'Feature': pd.DataFrame(X_train_scaled).columns,
    'Importance': best_rf.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nRandom Forest Feature Importance:")
print(feature_importance)

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.title('Random Forest Feature Importance')
plt.gca().invert_yaxis()
plt.show()

# Plot truth vs prediction
plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['ZHVI'], color='red', label='Truth (ZHVI)')
plt.plot(test['Year-Month'], test['RF_Predicted_ZHVI'], color='green', label='Random Forest Predicted ZHVI')
plt.xlabel('Date')
plt.ylabel('ZHVI')
x_ticks = np.arange(0, 80, 6)
plt.xticks(x_ticks)
plt.title('Time Series Plot: Truth vs Random Forest Predicted ZHVI')
plt.legend(loc='upper left')
plt.show()

# Optimized XGBoost Implementation with MAPE Focus

import xgboost as xgb
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import mean_absolute_percentage_error
import matplotlib.pyplot as plt
import math

# Feature Engineering Functions
def create_features(df, target_col='ZHVI'):
    # Create lag features
    for lag in [1, 2, 3, 6, 12]:  # Multiple time horizons
        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)

    # Create rolling statistics
    for window in [3, 6, 12]:
        df[f'{target_col}_rolling_avg_{window}'] = df[target_col].rolling(window).mean()
        df[f'{target_col}_rolling_std_{window}'] = df[target_col].rolling(window).std()

    # Month/year indicators
    df['month_sin'] = np.sin(2 * np.pi * df['Month']/12)
    df['month_cos'] = np.cos(2 * np.pi * df['Month']/12)

    return df

# Data Preparation
train = create_features(train.copy())
test = create_features(test.copy())

# Define features
base_features = ['Year', 'Month', 'TimeIndex', 'Unemployment Rate',
                'CPI', 'Interest Rate', 'GDP Growth', 'month_sin', 'month_cos']
lag_features = [col for col in train.columns if 'lag_' in col or 'rolling_' in col]
features = base_features + lag_features

# Handle missing values from lag features
X_train = train[features].dropna()
y_train = train.loc[X_train.index, 'ZHVI']
X_test = test[features].dropna()
y_test = test.loc[X_test.index, 'ZHVI']

# add polynomial features and scale
scaler = StandardScaler()
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)
X_train_scaled = scaler.fit_transform(X_train_poly)
X_test_scaled = scaler.transform(X_test_poly)

# XGBoost with Native API for MAPE Optimization
def xgboost_mape_train(X_train, y_train, X_test, y_test, params):
    # Convert to DMatrix format
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dtest = xgb.DMatrix(X_test, label=y_test)

    # Custom MAPE evaluation metric
    def mape_eval(preds, dmatrix):
        labels = dmatrix.get_label()
        return 'mape', np.mean(np.abs((labels - preds) / (labels + 1e-6))) * 100

    # Train model
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=1000,
        evals=[(dtrain, 'train'), (dtest, 'test')],
        early_stopping_rounds=50,
        feval=mape_eval,
        verbose_eval=50
    )
    return model

# Parameter Tuning with scikit-learn API
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.2],
    'min_child_weight': [1, 3, 5]
}

xgb_sklearn = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=100,
    random_state=42,
    n_jobs=-1
)

tscv = TimeSeriesSplit(n_splits=3)
search = RandomizedSearchCV(
    estimator=xgb_sklearn,
    param_distributions=param_grid,
    n_iter=20,
    cv=tscv,
    scoring='neg_mean_absolute_percentage_error',
    verbose=1,
    n_jobs=-1,
    random_state=42
)

search.fit(X_train_scaled, y_train)
best_params = search.best_params_

# Final Model Training with MAPE Focus
final_params = {
    **best_params,
    'objective': 'reg:squarederror',
    'seed': 158
}

# Train with native API
xgb_model = xgboost_mape_train(
    X_train_scaled, y_train,
    X_test_scaled, y_test,
    final_params
)

# Evaluation
xgb_predictions = xgb_model.predict(xgb.DMatrix(X_test_scaled))
test.loc[X_test.index, 'XGBoost_Predicted_ZHVI'] = xgb_predictions

metrics = {
    'RMSE': math.sqrt(mean_squared_error(y_test, xgb_predictions)),
    'MAPE': mean_absolute_percentage_error(y_test, xgb_predictions),
    'MAE': mean_absolute_error(y_test, xgb_predictions),
    'R2': r2_score(y_test, xgb_predictions)
}

print("\nModel Performance:")
for name, value in metrics.items():
    print(f"{name}: {value:.4f}")

# Plot truth vs prediction
plt.figure(figsize=(18, 6))
plt.plot(test['Year-Month'], test['ZHVI'], color='red', label='Truth (ZHVI)')
plt.plot(test['Year-Month'], test['XGBoost_Predicted_ZHVI'], color='darkgreen', label='XGBoost Predicted ZHVI')
plt.xlabel('Date')
plt.ylabel('ZHVI')
x_ticks = np.arange(0, 80, 6)
plt.xticks(x_ticks)
plt.title('Time Series Plot: Truth vs XGBoost Predicted ZHVI')
plt.legend(loc='upper left')
plt.show()

!pip install -q streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# 
# st.write('Hello, *World!* :sunglasses:')

!npm install localtunnel

!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com

import urllib
print("Password/Enpoint IP for localtunnel is:",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))